{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4a5e9a5e",
      "metadata": {
        "id": "4a5e9a5e"
      },
      "source": [
        "# Imports & Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "779b9c70",
      "metadata": {
        "id": "779b9c70",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-Worker_Count",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "cf88b954-f39a-412a-d87e-660833e735b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NAME          PLATFORM  WORKER_COUNT  PREEMPTIBLE_WORKER_COUNT  STATUS   ZONE           SCHEDULED_DELETE\r\n",
            "cluster-7a3b  GCE       5                                       RUNNING  us-central1-a\r\n"
          ]
        }
      ],
      "source": [
        "# if the following command generates an error, you probably didn't enable \n",
        "# the cluster security option \"Allow API access to all Google Cloud services\"\n",
        "# under Manage Security → Project Access when setting up the cluster\n",
        "!gcloud dataproc clusters list --region us-central1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "023d0b92",
      "metadata": {
        "id": "023d0b92",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-Setup",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "fc0e315d-21e9-411d-d69c-5b97e4e5d629"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install -q google-cloud-storage==1.43.0\n",
        "!pip install -q graphframes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1dfb4190",
      "metadata": {
        "id": "1dfb4190",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-Imports",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "a24aa24b-aa75-4823-83ca-1d7deef0f0de"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pyspark\n",
        "import sys\n",
        "from collections import Counter, OrderedDict, defaultdict\n",
        "import itertools\n",
        "from itertools import islice, count, groupby\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "from operator import itemgetter\n",
        "import nltk\n",
        "from nltk.stem.porter import *\n",
        "from nltk.corpus import stopwords\n",
        "from time import time\n",
        "from pathlib import Path\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import builtins\n",
        "import math\n",
        "from google.cloud import storage\n",
        "from collections import defaultdict\n",
        "from contextlib import closing\n",
        "import hashlib\n",
        "import json\n",
        "from inverted_index_gcp import InvertedIndex\n",
        "from inverted_index_gcp import MultiFileReader\n",
        "TUPLE_SIZE = 6\n",
        "client = storage.Client()\n",
        "nltk.download('stopwords')\n",
        "# adding our python module to the cluster\n",
        "sc.addFile(\"/home/dataproc/inverted_index_gcp.py\")\n",
        "sys.path.insert(0,SparkFiles.getRootDirectory())\n",
        "# if nothing prints here you forgot to upload the file inverted_index_gcp.py to the home dir\n",
        "%cd -q /home/dataproc\n",
        "full_path = \"gs://wikidata_preprocessed/*\"\n",
        "corpus = spark.read.parquet(full_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f54537d2",
      "metadata": {
        "id": "f54537d2",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-pyspark-import",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf, SparkFiles\n",
        "from pyspark.sql import SQLContext\n",
        "from graphframes import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fbc3e7e",
      "metadata": {
        "id": "2fbc3e7e",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-spark-version",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "07b4e22b-a252-42fb-fe46-d9050e4e7ca8",
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - hive</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://cluster-7a3b-m.us-central1-a.c.wikipediair.internal:39937\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.2</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>yarn</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>PySparkShell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f9611993460>"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e63056a",
      "metadata": {
        "id": "2e63056a"
      },
      "source": [
        "# Building an inverted index"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this part we bulid one function that bulid all the inverted indexes that we use for the information retrieval process."
      ],
      "metadata": {
        "id": "nTK_xZ_JXubn"
      },
      "id": "nTK_xZ_JXubn"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7f4e857",
      "metadata": {
        "id": "d7f4e857"
      },
      "outputs": [],
      "source": [
        "# All the function the userd in the process of bulid inverted index.\n",
        "\n",
        "RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n",
        "stemmer = PorterStemmer()\n",
        "all_stopwords = ['end','time','unit','use', 'known','new', 'was', 'how', 'for', 'could', 'please', 'then', 'whereupon', 'so', 'as', 'any', 'used', 'why', 'several', 'six', 'by', 'yourself', 'hence', 'did', 'whose', 'doing', 'n’t', 'show', 'her', 'yourselves', 'nothing', 'not', '‘ve', 'thereby', 'forty', 'each', 'whether', 'serious', 'perhaps', 'me', 'nevertheless', 'list', 'rather', 'elsewhere', 'former', 'seemed', 'up', 'than', 'except', 'via', 'can', 'everything', 'already', 'along', 'while', 'anyone', 'our', 'thus', 'must', 'either', 'move', '’ve', 'between', 'ever', 'does', 'whereafter', 'twelve', 'ca', 'in', 'again', 'itself', 'themselves', 'until', 'whence', 'she', 'us', 'say', 'no', 'amongst', 'were', 'whole', 'nine', 'under', 'everywhere', 'an', 'therein', 'go', 'would', 'beyond', 'become', 's', 'category', 'sixty', 'myself', 'they', 'about', 'since', 'put', 'amount', 'whatever', 'whereby', 'though', 'thru', 'a', \"'ll\", 'never', 'their', 'even', 'ours', 'twenty', 'am', 'someone', \"'s\", 'that', 'last', 'to', 'becoming', 'upon', 'one', 'this', 'meanwhile', 'more', 'else', 'himself', 'moreover', 'bottom', 'well', 'became', 'same', 'alone', '‘d', 'these', 'before', 'with', 'but', 'because', 'following', 'hers', 'really', 'thereupon', 'third', 'always', 'some', \"'ve\", 'hereby', 'his', 'above', 'thumb', 'part', 'few', 'formerly', \"'d\", '’d', 'however', 'history', 'seeming', 'having', 'somewhere', 'him', 'neither', 'do', 'whereas', 'besides', 'wherein', 'next', 'throughout', 'against', 'is', 'unless', 'yet', 'therefore', 'many', 'eight', 'still', 'name', 'nobody', 'top', 'mine', 'somehow', 'you', 'been', 'fifty', 'where', 'sometimes', 'get', 'hereafter', 'three', 'quite', 'towards', 'made', 'may', 'side', 'thereafter', 'beside', 'noone', 'none', 'be', 'anywhere', 'further', '‘ll', 'links', 'toward', 'due', 'my', 'what', 'references', 'beforehand', 'there', 'thence', 'together', 'four', 'among', 'at', \"n't\", 'such', 't', 'external', 'people', 'using', 'something', 'ourselves', 'keep', 'another', 'onto', 'latterly', '‘s', 'had', 'or', 'only', 'hundred', 'whenever', 'also', 'see', '‘re', 'whither', 'less', 'much', 'others', 'should', 'sometime', 'although', 'around', 'seem', 'of', 'down', 'five', 'them', 'on', '‘m', 'yours', 'very', 'various', 'empty', 'through', 'wherever', 'most', 'cannot', 'here', 'namely', 'mostly', 'per', 'during', 'whoever', 'out', 'we', 'second', '’re', 'both', 're', 'ten', 'don', 'anyhow', 'after', 'back', 'everyone', 'done', 'often', 'call', 'latter', 'seems', 'might', 'now', 'take', 'fifteen', 'every', '’m', 'own', 'becomes', 'all', 'make', 'which', 'full', 'indeed', 'nor', 'give', 'other', 'almost', 'hereupon', 'who', 'nowhere', '’s', 'including', \"'re\", 'too', 'i', 'he', 'its', 'those', 'the', 'it', 'first', 'herein', '’ll', 'just', 'over', 'have', 'and', \"'m\", 'when', 'eleven', 'below', 'if', 'theirs', 'behind', 'whom', 'are', 'will', 'from', 'into', 'enough', 'your', 'least', 'anyway', 'n‘t', 'herself', 'has', 'within', 'once', 'front', 'anything', 'afterwards', 'being', 'off', 'across', 'two', 'otherwise', 'regarding', 'without']\n",
        "\n",
        "# Convert word to number\n",
        "def _hash(s):\n",
        "    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n",
        "\n",
        "# Tokenize the each text / title and remove stop word. If ngram = True, concatenate adjacent words.\n",
        "def get_tokens(text, ngram=False):\n",
        "    tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n",
        "    tokens = [stemmer.stem(term) for term in tokens if term not in all_stopwords]\n",
        "    if not ngram: return tokens\n",
        "    if len(tokens)<2: return []\n",
        "    b = Counter([''.join(x) for x in zip(tokens[0::1], tokens[1::1])])\n",
        "    return [i[0] for i in b.most_common()][:10]\n",
        "\n",
        "# Count number of apperance of word in text.\n",
        "def word_count(text, id, ngram=False):\n",
        "    tokens = get_tokens(text, ngram)\n",
        "    words_counter = Counter(tokens)\n",
        "    return [(word,(id,words_counter[word])) for word in words_counter]\n",
        "\n",
        "# Count the number of word in text.\n",
        "def len_doc(text, id, ngram=False):\n",
        "    tokens = get_tokens(text, ngram)\n",
        "    return (id, len(tokens))\n",
        "\n",
        "# Calculate the size in the TD-IDF dimension.\n",
        "def size_doc(text, id ,df ,corpus_size, ngram=False):\n",
        "    tokens = get_tokens(text, ngram)\n",
        "    words_counter = Counter(tokens)\n",
        "    size = builtins.sum([(c/len(tokens)*math.log2(corpus_size/df[w]))**2 for w,c in words_counter.items() if w in df])\n",
        "    return (id, math.sqrt(size))\n",
        "\n",
        "# Sorting posting list, key is the term frequency in each doc.\n",
        "def reduce_word_counts(unsorted_pl):\n",
        "    return sorted(unsorted_pl, key = lambda x: x[1], reverse = True)\n",
        "\n",
        "# Hash function the convert each token to bucket\n",
        "def token2bucket_id(token, NUM_BUCKETS):\n",
        "    return int(_hash(token),16) % NUM_BUCKETS\n",
        "\n",
        "# Write postind lists to buckets.\n",
        "def partition_postings_and_write(postings, bucket_name, NUM_BUCKETS):\n",
        "    ps = postings.map(lambda x: (token2bucket_id(x[0], NUM_BUCKETS),[x]))\n",
        "    ps = ps.reduceByKey(lambda x, y: x+y)\n",
        "    ps = ps.mapValues(reduce_PLs)\n",
        "    return ps.map(lambda x: InvertedIndex.write_a_posting_list((x[0], x[1]), bucket_name))\n",
        "\n",
        "# Sort posting list for each bucket by the size of the posting list.\n",
        "def reduce_PLs(unsorted_pl):\n",
        "    sorted_Pl =  sorted(unsorted_pl, key = lambda x: len(x[1]), reverse = True)\n",
        "    return [(i[0],i[1][:200]) for i in sorted_Pl]\n",
        "\n",
        "# Calculate doc frequency of each word.\n",
        "def calculate_df(postings):\n",
        "    return postings.reduceByKey(lambda posting_list1, posting_list2: posting_list1+posting_list2)\\\n",
        "                .mapValues(lambda x: len(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e44955c",
      "metadata": {
        "id": "8e44955c"
      },
      "outputs": [],
      "source": [
        "# Main function that creat each inverted index.\n",
        "# params :\n",
        "# corpus : wikipedia corpus\n",
        "# bucket_name : the name of the bucket to store the inverted index\n",
        "# NUM_BUCKETS : num of bins in the bucket\n",
        "# filter_size : filter size for rare words\n",
        "# ngram : bool to do the inverdet bgram or unigram\n",
        "def Create_Inverted_Index(corpus, bucket_name, column, NUM_BUCKETS,  filter_size = 0, ngram=False):\n",
        "  doc_pairs = corpus.select(column, \"id\").rdd\n",
        "  word_counts = doc_pairs.flatMap(lambda x: word_count(x[0], x[1],ngram))\n",
        "  postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n",
        "  # postings = postings.sortBy(lambda a: -len(a[1])).take(2000000)\n",
        "  postings_filtered = postings.filter(lambda x: len(x[1])>filter_size and len(x[1])<900000)\n",
        "  posting_locs_list = partition_postings_and_write(postings_filtered, bucket_name, NUM_BUCKETS).collect()\n",
        "  w2df = calculate_df(postings)\n",
        "#   id_len = doc_pairs.map(lambda x: len_doc(x[0], x[1],ngram))\n",
        "  super_posting_locs = defaultdict(list)\n",
        "  for blob in client.list_blobs(bucket_name, prefix='postings_gcp'):\n",
        "    if not blob.name.endswith(\"pickle\"):\n",
        "      continue\n",
        "    with blob.open(\"rb\") as f:\n",
        "      posting_locs = pickle.load(f)\n",
        "    for k, v in posting_locs.items():\n",
        "       super_posting_locs[k].extend(v)\n",
        "  inverted = InvertedIndex(bucket_name)\n",
        "  inverted.posting_locs = super_posting_locs\n",
        "  w2df_dict = w2df.collectAsMap()\n",
        "  inverted.df = w2df_dict\n",
        "#   inverted.DL = dict(id_len.collect())\n",
        "#   id_size = doc_pairs.map(lambda x: size_doc(x[0], x[1], w2df_dict, len(inverted.DL), ngram))\n",
        "#   inverted.DS = dict(id_size.collect())\n",
        "  inverted.write_index('.', f'index_{bucket_name}')\n",
        "  index_src = f\"index_{bucket_name}.pkl\"\n",
        "  index_dst = f'gs://{bucket_name}/postings_gcp/{index_src}'\n",
        "  !gsutil cp $index_src $index_dst"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bba412a",
      "metadata": {
        "id": "1bba412a",
        "outputId": "f46e6cf9-5357-4566-ffbe-ea430d43f2fa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "#Create inverted indexes\n",
        "Create_Inverted_Index(corpus, 'title_inverted_index', 'title', 124, filter_size = 0)\n",
        "Create_Inverted_Index(corpus, 'body_inverted_index', 'text', 248, filter_size = 70) \n",
        "Create_Inverted_Index(corpus, 'title2_inverted_index', 'title', 124, filter_size = 0, ngram=True)\n",
        "Create_Inverted_Index(corpus, 'body2_inverted_index', 'text', 124, filter_size = 50, ngram=True)\n",
        "Create_Inverted_Index(corpus, 'simple_title_inverted_index', 'title', 124, filter_size = 50)\n",
        "Create_Inverted_Index(corpus, 'simple_body_inverted_index', 'text', 124, filter_size = 50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86b303ea",
      "metadata": {
        "id": "86b303ea"
      },
      "source": [
        "# Create Page rank"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section we build the Page Rank of the corpus."
      ],
      "metadata": {
        "id": "2ZdIvLQ4bVRc"
      },
      "id": "2ZdIvLQ4bVRc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ff53cae",
      "metadata": {
        "id": "4ff53cae"
      },
      "outputs": [],
      "source": [
        "def generate_graph(pages):\n",
        "  edges = pages.flatMap(lambda x: map(lambda y: (x[0], y[0]), x[1])).distinct()\n",
        "  vertices = edges.flatMap(lambda x: x).distinct().map(lambda x:(x,))\n",
        "  return edges, vertices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33b8f944",
      "metadata": {
        "id": "33b8f944"
      },
      "outputs": [],
      "source": [
        "pages_links = corpus.select(\"id\", \"anchor_text\").rdd\n",
        "# construct the graph \n",
        "edges, vertices = generate_graph(pages_links)\n",
        "# compute PageRank\n",
        "edgesDF = edges.toDF(['src', 'dst']).repartition(124, 'src')\n",
        "verticesDF = vertices.toDF(['id']).repartition(124, 'id')\n",
        "g = GraphFrame(verticesDF, edgesDF)\n",
        "pr_results = g.pageRank(resetProbability=0.15, maxIter=6)\n",
        "pr = pr_results.vertices.select(\"id\", \"pagerank\")\n",
        "pr = pr.sort(col('pagerank').desc())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf7fe2fb",
      "metadata": {
        "id": "cf7fe2fb"
      },
      "outputs": [],
      "source": [
        "# Store the page rank in bucket\n",
        "pr.repartition(1).write.csv('gs://title_inverted_index/pr', compression=\"gzip\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Anchor inverted index"
      ],
      "metadata": {
        "id": "ZgIl6ia_cPD2"
      },
      "id": "ZgIl6ia_cPD2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section we create the anchor inverted index"
      ],
      "metadata": {
        "id": "CLSrHALrcUPV"
      },
      "id": "CLSrHALrcUPV"
    },
    {
      "cell_type": "code",
      "source": [
        "bucket_name = 'anchor_bucket'\n",
        "anchor_InvertedIndex = InvertedIndex(bucket_name)"
      ],
      "metadata": {
        "id": "_nwYGPpScsfY"
      },
      "id": "_nwYGPpScsfY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pages_titles = corpus(full_path).select(\"id\", 'title').rdd\n",
        "pages_links = corpus(full_path).select(\"anchor_text\").rdd\n",
        "pg = pages_links.flatMap(lambda x:[(get_tokens(text), id) for id, text in x[0]])\n",
        "postings = pg.flatMap(lambda x: [(y,x[1]) for  y in x[0]]).groupByKey()\n",
        "anchor_posting = postings.mapValues(list).map(lambda x: (x[0], Counter(x[1]).most_common()))\n",
        "postings_filtered = anchor_posting.filter(lambda x: len(x[1]) > 50).mapValues(lambda x: x[:50])\n",
        "anchor_InvertedIndex.DL = pages_titles.map(lambda x: (x[0], x[1])).collectAsMap()\n",
        "anchor_InvertedIndex.df = Counter(calculate_df(postings_filtered).collectAsMap())\n",
        "posting_locs_list = partition_postings_and_write(postings_filtered, bucket_name, 124).collect()"
      ],
      "metadata": {
        "id": "DkOKaV3ecZam"
      },
      "id": "DkOKaV3ecZam",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# collect all posting lists locations into one super-set\n",
        "super_posting_locs_text = defaultdict(list)\n",
        "for blob in client.list_blobs(bucket_name, prefix='postings_gcp'):\n",
        "  if not blob.name.endswith(\"pickle\"):\n",
        "    continue\n",
        "  with blob.open(\"rb\") as f:\n",
        "    posting_locs = pickle.load(f)\n",
        "    for k, v in posting_locs.items():\n",
        "      super_posting_locs_text[k].extend(v)\n",
        "anchor_InvertedIndex.posting_locs = super_posting_locs_text"
      ],
      "metadata": {
        "id": "KlVIGOJadW_4"
      },
      "id": "KlVIGOJadW_4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# write the global stats out\n",
        "anchor_InvertedIndex.write_index('.', f'index_{bucket_name}')\n",
        "# upload to gs\n",
        "inverted.write_index('.', f'index_{bucket_name}')\n",
        "index_src = f\"index_{bucket_name}.pkl\"\n",
        "index_dst = f'gs://{bucket_name}/postings_gcp/{index_src}'\n",
        "!gsutil cp $index_src $index_dst"
      ],
      "metadata": {
        "id": "bBV23hb-dkwR"
      },
      "id": "bBV23hb-dkwR",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "celltoolbar": "Create Assignment",
    "colab": {
      "collapsed_sections": [
        "4a5e9a5e",
        "86b303ea",
        "ZgIl6ia_cPD2"
      ],
      "name": "Create_inveted_indexes.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "PySpark",
      "language": "python",
      "name": "pyspark"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}